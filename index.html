<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">

  <!-- Face Mesh ライブラリを読み込む -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
</head>
<body>
  <div class="container">
    
    <!-- Webカメラの映像（入力） -->
    <video id="input"></video>
    
    <!--  認識した顔メッシュを可視化した映像（出力）  -->
    <canvas id="output" width="600" height="400"></canvas>
    
    <!-- 認識結果を表示するエリア -->
    <div id="result"></div> 
  </div>
  <button id="start">start</button>
  <button id="stop">stop</button>
  
  <script>
    const video = document.getElementById('input');
    const canvas = document.getElementById('output');
    const ctx = canvas.getContext('2d');
    const resultDiv = document.getElementById('result'); // 結果表示エリアの取得

    //関連ファイルの読み込み (Face Mesh に変更)
    const config = {
      locateFile: file => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
    };
    const faceMesh = new FaceMesh(config);

    //カメラからの映像をfaceMesh.jsで使えるようにする
    const camera = new Camera(video, {
      onFrame: async () => {
        await faceMesh.send({image: video});
      },
      width: 600,
      height: 400
    });

    faceMesh.setOptions({
        maxNumFaces: 1,              //検出する顔の最大数
        minDetectionConfidence: 0.5, //顔を検出するための信頼値(0.0~1.0)
        minTrackingConfidence: 0.5   //ランドマーク追跡の信頼度(0.0~1.0)
    });

    //顔メッシュ認識結果の取得
    faceMesh.onResults(results => {
      ctx.clearRect(0,0,canvas.width,canvas.height);
      ctx.drawImage(results.image,0,0,canvas.width,canvas.height);
      
      let winkDetected = false; // ウィンク検出フラグ
      let mouthOpenDetected = false; // 口の開き検出フラグ

      if(results.multiFaceLandmarks) {
        for (const landmarks of results.multiFaceLandmarks) {
          drawConnectors(ctx, landmarks, FACEMESH_TESSELATION, {color: '#C0C0C070', lineWidth: 1});
          drawConnectors(ctx, landmarks, FACEMESH_RIGHT_EYE, {color: '#FF3030'});
          drawConnectors(ctx, landmarks, FACEMESH_RIGHT_EYEBROW, {color: '#FF3030'});
          drawConnectors(ctx, landmarks, FACEMESH_LEFT_EYE, {color: '#30FF30'});
          drawConnectors(ctx, landmarks, FACEMESH_LEFT_EYEBROW, {color: '#30FF30'});
          drawConnectors(ctx, landmarks, FACEMESH_FACE_OVAL, {color: '#E0E0E0'});
          drawConnectors(ctx, landmarks, FACEMESH_LIPS, {color: '#E0E0E0'});

          // ウィンクの認識 (参考: ランドマークのインデックスは調整が必要な場合があります)
          const upperEyeLid = landmarks[159];
          const lowerEyeLid = landmarks[145];
          const eyeDistance = Math.sqrt(
            Math.pow(upperEyeLid.x - lowerEyeLid.x, 2) + 
            Math.pow(upperEyeLid.y - lowerEyeLid.y, 2)
          );

          if (eyeDistance < 0.015) { // 閾値: 調整が必要
            winkDetected = true; 
          }

          // 口の開きの認識 (参考: ランドマークのインデックスは調整が必要な場合があります)
          const upperLip = landmarks[13];
          const lowerLip = landmarks[14];
          const mouthDistance = Math.sqrt(
            Math.pow(upperLip.x - lowerLip.x, 2) + 
            Math.pow(upperLip.y - lowerLip.y, 2)
          );
          
          if (mouthDistance > 0.04) { // 閾値: 調整が必要
            mouthOpenDetected = true;
          }
        }
      }
      
      // 認識結果を表示
      if (winkDetected) {
        resultDiv.innerHTML = "ウィンクを検出しました！";
      } else if (mouthOpenDetected) {
        resultDiv.innerHTML = "口を大きく開けるのを検出しました！";
      } else {
        resultDiv.innerHTML = "";
      }

    });

    
    //認識開始・終了ボタン
    document.getElementById('start')
      .addEventListener('click', () => camera.start());
    document.getElementById('stop')
      .addEventListener('click', () =>  camera.stop());

  </script>
</body>
</html>